{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS235 Project - Fall 2024\n",
    "- Alex Cupps (acupp002)\n",
    "- Zach Schwartz (zschw004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell reserved for all needed imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to analyze our data, we must first load it from the .data file provided, along with assigning the column names for the respective features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID Diagnosis  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
      "0    842302         M    17.99     10.38      122.80  1001.0      0.11840   \n",
      "1    842517         M    20.57     17.77      132.90  1326.0      0.08474   \n",
      "2  84300903         M    19.69     21.25      130.00  1203.0      0.10960   \n",
      "3  84348301         M    11.42     20.38       77.58   386.1      0.14250   \n",
      "4  84358402         M    20.29     14.34      135.10  1297.0      0.10030   \n",
      "\n",
      "   compactness1  concavity1  concave_points1  ...  radius3  texture3  \\\n",
      "0       0.27760      0.3001          0.14710  ...    25.38     17.33   \n",
      "1       0.07864      0.0869          0.07017  ...    24.99     23.41   \n",
      "2       0.15990      0.1974          0.12790  ...    23.57     25.53   \n",
      "3       0.28390      0.2414          0.10520  ...    14.91     26.50   \n",
      "4       0.13280      0.1980          0.10430  ...    22.54     16.67   \n",
      "\n",
      "   perimeter3   area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
      "0      184.60  2019.0       0.1622        0.6656      0.7119           0.2654   \n",
      "1      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
      "2      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
      "3       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
      "4      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
      "\n",
      "   symmetry3  fractal_dimension3  \n",
      "0     0.4601             0.11890  \n",
      "1     0.2750             0.08902  \n",
      "2     0.3613             0.08758  \n",
      "3     0.6638             0.17300  \n",
      "4     0.2364             0.07678  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['ID',\n",
    "           'Diagnosis',\n",
    "           'radius1',\n",
    "           'texture1',\n",
    "           'perimeter1',\n",
    "           'area1',\n",
    "           'smoothness1',\n",
    "           'compactness1',\n",
    "           'concavity1',\n",
    "           'concave_points1',\n",
    "           'symmetry1',\n",
    "           'fractal_dimension1',\n",
    "           'radius2',\n",
    "           'texture2',\n",
    "           'perimeter2',\n",
    "           'area2',\n",
    "           'smoothness2',\n",
    "           'compactness2',\n",
    "           'concavity2',\n",
    "           'concave_points2',\n",
    "           'symmetry2',\n",
    "           'fractal_dimension2',\n",
    "           'radius3',\n",
    "           'texture3',\n",
    "           'perimeter3',\n",
    "           'area3',\n",
    "           'smoothness3',\n",
    "           'compactness3',\n",
    "           'concavity3',\n",
    "           'concave_points3',\n",
    "           'symmetry3',\n",
    "           'fractal_dimension3'\n",
    "           ]\n",
    "\n",
    "data = pd.read_csv('data/wdbc.data', header=None, sep=',', names=columns)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implementing simple classifiers [15pts]\n",
    "- What to implement: In this question you should implement (1) a decision tree classifier\n",
    "that uses the Information Gain splitting criterion and (2) a Naive Bayes classifier which uses\n",
    "Gaussian modeling for continuous features.\n",
    "- What to plot: You should produce a bar-chart that shows the performance of your classifiers\n",
    "on the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a decision tree classifier using the information gain splitting criterion to effectively utilize the feature at each node that provides us with the most information gain about our target feature, 'Diagnosis' - telling us whether or not a breast mass is malignant (M) or benign (B). \n",
    "<br><br>\n",
    "First, we must prepare the data for the decision tree. First, we will drop the ID column and data from our dataset, as this data is not relevant to our predictive model. Next, we will create two new variables:\n",
    "1. features - the data for all features in our dataset other than the target\n",
    "2. target - the target feature of our dataset (Diagnosis) - determining whether a tumor is benign or malignant. We will convert the diagnosis data from categorical to numerical for the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ID' in data.columns:\n",
    "    data = data.drop(columns=['ID'])\n",
    "    \n",
    "features = data.drop(columns=['Diagnosis']).values\n",
    "target = data['Diagnosis'].map({'M': 1, 'B': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a function 'calculate_entropy' that takes the various values in the dataset and calculate the entropy, or level of purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(x):\n",
    "    counts = np.bincount(x)\n",
    "    probabilities = counts / len(x)\n",
    "    entropy = 0\n",
    "\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * abs(np.log2(prob))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our entropy calculation completed, we need to create a function for calculating information gain. Since our features are mainly continuous, we should not perform exact value splits - doing so would lead to many small (and mostly single-item) bins, which would result in overfitting of our model. Instead, we will come up with thresholds between values for our splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best information gain: 0.31127812445913283\n",
      "Best threshold: 1.75\n"
     ]
    }
   ],
   "source": [
    "def calculate_information_gain(dataset, labels, feature):\n",
    "    \n",
    "    #sort the indexes of the dataset for the given feature\n",
    "    sorted = np.argsort(dataset[:, feature])\n",
    "\n",
    "    #apply the sorting to the feature data and labels\n",
    "    sorted_data = dataset[sorted]\n",
    "    sorted_labels = labels[sorted]\n",
    "\n",
    "    #start at the top, calculating entropy level of the dataset\n",
    "    #to determine the level of purity\n",
    "    parent_entropy = calculate_entropy(labels)\n",
    "\n",
    "    #iterate over possible thresholds and get the indexes on each side\n",
    "    top_info_gain = 0\n",
    "    top_threshold = None\n",
    "    for i in range(1, len(sorted_data)):\n",
    "\n",
    "        threshold = (sorted_data[i-1, feature] + sorted_data[i, feature]) / 2\n",
    "        #left split is a boolean array containing True for the indexes with a value less than the threshold\n",
    "        #right split is a boolean array containing True for the indexes with a value greater than or equal to the threshold\n",
    "        left_split = sorted_data[:, feature] < threshold\n",
    "        right_split = sorted_data[:, feature] >= threshold\n",
    "\n",
    "        #next we calculate the entropies for the left and right sides of the split\n",
    "        #this will tell us how effective the split is, and we will maximize information gain\n",
    "        left_split_entropy = calculate_entropy(sorted_labels[left_split])\n",
    "        right_split_entropy = calculate_entropy(sorted_labels[right_split])\n",
    "\n",
    "        #calculate the weights of the left and right splits for use in the overall weight entropy calculation\n",
    "        weight_l = np.sum(left_split) / len(sorted_data)\n",
    "        weight_r = np.sum(right_split) / len(sorted_data)\n",
    "\n",
    "        #calculate the overall weighted entropy of the current split\n",
    "        weighted_split_entropy = weight_l * left_split_entropy + weight_r * right_split_entropy\n",
    "\n",
    "        #using the weighted split entropy, calculate the information gain of the threshold\n",
    "        #if the information gain is higher than any we've come across thus far, set as the top one\n",
    "        information_gain = parent_entropy - weighted_split_entropy\n",
    "        if information_gain > top_info_gain:\n",
    "            top_info_gain = information_gain\n",
    "            top_threshold = threshold\n",
    "\n",
    "    return top_info_gain, top_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our functions to calculate entropy and weighted information gain, we will create the Decision Tree Classifier itself. First, we need to create a Node class to store various attributes (feature index, threshold, left of threshold values, right of threshold values) of the node we are evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "\n",
    "        #feature: index of feature being used for split\n",
    "        self.feature = feature\n",
    "\n",
    "        #threshold: threshold for the split\n",
    "        self.threshold = threshold\n",
    "\n",
    "        #left: the left node, contains all values < threshold\n",
    "        self.left = left\n",
    "\n",
    "        #right: the right node, contains all values >= threshold\n",
    "        self.right = right\n",
    "\n",
    "        #value: value (0 or 1 for benign or malignant) assigned when a leaf node is reached\n",
    "        #remains None until leaf node is found, then returns the value\n",
    "        self.value = value\n",
    "\n",
    "    def leaf_node_reached(self):\n",
    "        if self.value is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our DecisionTreeClassifier class. This object contains all relevant functions for building our decision tree, recursively building tree nodes and calculating/maximizing information gain. I've annotated each function below with comments on its purpose: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, maximum_tree_depth):\n",
    "\n",
    "        #maximum_tree_depth (int): the maximum depth for our tree\n",
    "        self.maximum_tree_depth = maximum_tree_depth\n",
    "\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, dataset, labels):\n",
    "        #function containing all logic to train our classifier\n",
    "        self.tree = self.build_tree(dataset, labels)\n",
    "\n",
    "    def build_tree(self, dataset, labels, cur_tree_depth=0):\n",
    "        #this function is where the bulk of the work is handled for building our decision tree\n",
    "\n",
    "        #label count gives us the number of labels in the current sample set\n",
    "        #label count of 1 indicates perfect purity (one of our stopping criteria)\n",
    "        label_ct = len(np.unique(labels))\n",
    "\n",
    "        #we first check our stoppage criteria, to see if we've reached a leaf node\n",
    "        #if our current depth has hit the stoppage depth defined, OR\n",
    "        #if the label count has a value of 1\n",
    "        if cur_tree_depth >= self.maximum_tree_depth or label_ct == 1:\n",
    "            # Return a leaf node with the most common class label\n",
    "            leaf_value = self.get_top_label(labels)\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        #call get_top_split() to calculate the top feature and threshold\n",
    "        top_feature, top_threshold = self.get_top_split(dataset, labels)\n",
    "        if top_feature is None:\n",
    "            #no feature being returned indicates we've reached a leaf node\n",
    "            #return this TreeNode object with the leaf value found\n",
    "            leaf_value = self.get_top_label(labels)\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        #split data on top feature and threshold\n",
    "        #then build the sub trees off using this split\n",
    "        left_split = dataset[:, top_feature] < top_threshold\n",
    "        right_split = dataset[:, top_feature] >= top_threshold\n",
    "        left_sub_tree = self.build_tree(dataset[left_split], labels[left_split], cur_tree_depth + 1)\n",
    "        right_sub_tree = self.build_tree(dataset[right_split], labels[right_split], cur_tree_depth + 1)\n",
    "\n",
    "        return TreeNode(feature=top_feature, threshold=top_threshold, left=left_sub_tree, right=right_sub_tree)\n",
    "\n",
    "    def get_top_split(self, dataset, labels):\n",
    "\n",
    "        feature_ct = dataset.shape[1]\n",
    "        top_feature = None\n",
    "        top_threshold = None\n",
    "        top_info_gain = 0\n",
    "\n",
    "        #iterate over the features and determine which feature/threshold provide the highest information gain\n",
    "        for feature in range(feature_ct):\n",
    "            info_gain, threshold = calculate_information_gain(dataset, labels, feature)\n",
    "            if info_gain > top_info_gain:\n",
    "                top_info_gain = info_gain\n",
    "                top_feature = feature\n",
    "                top_threshold = threshold\n",
    "\n",
    "        return top_feature, top_threshold\n",
    "\n",
    "    def get_top_label(self, labels):\n",
    "        #returns the most common label in the array\n",
    "        #we call this function to determine whether benign or malignant samples are more prominent\n",
    "        return np.bincount(labels).argmax()\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        predictions = []\n",
    "        for point in dataset:\n",
    "            #append the predicted label value to our predictions list\n",
    "            predictions.append(self.predict_datapoint(point, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_datapoint(self, point, node):\n",
    "        #first check if we've reached a leaf node\n",
    "        #if so, return the determined label value\n",
    "        if node.leaf_node_reached():\n",
    "            return node.value\n",
    "        \n",
    "        #if not leaf value, continue down the left or right tree nodes\n",
    "        if point[node.feature] < node.threshold:\n",
    "            return self.predict_datapoint(point, node.left)\n",
    "        else:\n",
    "            return self.predict_datapoint(point, node.right)\n",
    "\n",
    "    #required getter and setter functions for our object \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"maximum_tree_depth\": self.maximum_tree_depth}\n",
    "\n",
    "    def set_params(self, maximum_tree_depth=None):\n",
    "        self.maximum_tree_depth = maximum_tree_depth\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our decision tree classifier, we will begin the process of testing the performance and analyzing our results. Below, we've created two functions:\n",
    "\n",
    "1. test_performance() - this function has two arguments, X (the data matrix) and y (the label values). \n",
    "2. get_f1() - a helper function called by our test_performance function. This function takes four arguments - the data matrix and label values passed in from test_performance, along with the stratified k fold and classifier objects we've created. We return sklearn's cross_val_score call on our parameters, which returns an array containing the f1 scores at each fold in the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(X, y):\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    classifier = DecisionTreeClassifier(maximum_tree_depth=5)\n",
    "\n",
    "    f1_scores = get_f1(X, y, kfold, classifier)\n",
    "\n",
    "    mean_f1_scores = np.mean(f1_scores)\n",
    "    stdev_f1_scores = np.std(f1_scores)\n",
    "\n",
    "    return mean_f1_scores, stdev_f1_scores\n",
    "\n",
    "def get_f1(X, y, skf, classifier):\n",
    "    return cross_val_score(classifier, X, y, cv=skf, scoring=make_scorer(f1_score, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've built our performance calculating functions, we want to plot our results. We call test_performance, and get the mean and standard deviation values of our f1 scores at each fold, for both of our classifiers. With these two values, we build up a plot to show each classifier's performance, along with the levels of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Mean: 0.9289349689853115\n",
      "Decision Tree F1 Std. Dev.: 0.039723431473311976\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dec_tree_mean_f1, dec_tree_std_f1 = test_performance(features, target)\n",
    "print(f\"Decision Tree F1 Mean: {dec_tree_mean_f1}\")\n",
    "print(f\"Decision Tree F1 Std. Dev.: {dec_tree_std_f1}\")\n",
    "\n",
    "def plot_performance(mean_f1, std_f1):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar('Decision Tree', mean_f1, yerr=std_f1, color='blue', capsize=10)\n",
    "    ax.set_ylabel('F1 Score (mean)')\n",
    "    ax.set_title('Classifier Performance')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(dec_tree_mean_f1, dec_tree_std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we can see that the decision tree classifier has roughly a 93% mean F1 score, which indicates our classifier is effectively handling our imbalanced dataset and considering false positives and false negatives. The low standard deviation (roughly 0.03) tells us that the classifier is avoiding overfitting the data and performing well across the ten folds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
