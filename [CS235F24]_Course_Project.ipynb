{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS235 Project - Fall 2024\n",
    "- Alex Cupps (acupp002)\n",
    "- Zach Schwartz (zschw004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell reserved for all needed imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to analyze our data, we must first load it from the .data file provided, along with assigning the column names for the respective features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID Diagnosis  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
      "0    842302         M    17.99     10.38      122.80  1001.0      0.11840   \n",
      "1    842517         M    20.57     17.77      132.90  1326.0      0.08474   \n",
      "2  84300903         M    19.69     21.25      130.00  1203.0      0.10960   \n",
      "3  84348301         M    11.42     20.38       77.58   386.1      0.14250   \n",
      "4  84358402         M    20.29     14.34      135.10  1297.0      0.10030   \n",
      "\n",
      "   compactness1  concavity1  concave_points1  ...  radius3  texture3  \\\n",
      "0       0.27760      0.3001          0.14710  ...    25.38     17.33   \n",
      "1       0.07864      0.0869          0.07017  ...    24.99     23.41   \n",
      "2       0.15990      0.1974          0.12790  ...    23.57     25.53   \n",
      "3       0.28390      0.2414          0.10520  ...    14.91     26.50   \n",
      "4       0.13280      0.1980          0.10430  ...    22.54     16.67   \n",
      "\n",
      "   perimeter3   area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
      "0      184.60  2019.0       0.1622        0.6656      0.7119           0.2654   \n",
      "1      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
      "2      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
      "3       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
      "4      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
      "\n",
      "   symmetry3  fractal_dimension3  \n",
      "0     0.4601             0.11890  \n",
      "1     0.2750             0.08902  \n",
      "2     0.3613             0.08758  \n",
      "3     0.6638             0.17300  \n",
      "4     0.2364             0.07678  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['ID',\n",
    "           'Diagnosis',\n",
    "           'radius1',\n",
    "           'texture1',\n",
    "           'perimeter1',\n",
    "           'area1',\n",
    "           'smoothness1',\n",
    "           'compactness1',\n",
    "           'concavity1',\n",
    "           'concave_points1',\n",
    "           'symmetry1',\n",
    "           'fractal_dimension1',\n",
    "           'radius2',\n",
    "           'texture2',\n",
    "           'perimeter2',\n",
    "           'area2',\n",
    "           'smoothness2',\n",
    "           'compactness2',\n",
    "           'concavity2',\n",
    "           'concave_points2',\n",
    "           'symmetry2',\n",
    "           'fractal_dimension2',\n",
    "           'radius3',\n",
    "           'texture3',\n",
    "           'perimeter3',\n",
    "           'area3',\n",
    "           'smoothness3',\n",
    "           'compactness3',\n",
    "           'concavity3',\n",
    "           'concave_points3',\n",
    "           'symmetry3',\n",
    "           'fractal_dimension3'\n",
    "           ]\n",
    "\n",
    "data = pd.read_csv('data/wdbc.data', header=None, sep=',', names=columns)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implementing simple classifiers [15pts]\n",
    "- What to implement: In this question you should implement (1) a decision tree classifier\n",
    "that uses the Information Gain splitting criterion and (2) a Naive Bayes classifier which uses\n",
    "Gaussian modeling for continuous features.\n",
    "- What to plot: You should produce a bar-chart that shows the performance of your classifiers\n",
    "on the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a decision tree classifier using the information gain splitting criterion to effectively utilize the feature at each node that provides us with the most information gain about our target feature, 'Diagnosis' - telling us whether or not a breast mass is malignant (M) or benign (B). \n",
    "<br><br>\n",
    "First, we must prepare the data for the decision tree. First, we will drop the ID column and data from our dataset, as this data is not relevant to our predictive model. Next, we will create two new variables:\n",
    "1. features - the data for all features in our dataset other than the target\n",
    "2. target - the target feature of our dataset (Diagnosis) - determining whether a tumor is benign or malignant. We will convert the diagnosis data from categorical to numerical for the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ID' in data.columns:\n",
    "    data = data.drop(columns=['ID'])\n",
    "    \n",
    "features = data.drop(columns=['Diagnosis']).values\n",
    "target = data['Diagnosis'].map({'M': 1, 'B': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a function 'calculate_entropy' that takes the various values in the dataset and calculate the entropy, or level of purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(x):\n",
    "    counts = np.bincount(x)\n",
    "    probabilities = counts / len(x)\n",
    "    entropy = 0\n",
    "\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * abs(np.log2(prob))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our entropy calculation completed, we need to create a function for calculating information gain. Since our features are mainly continuous, we should not perform exact value splits - doing so would lead to many small (and mostly single-item) bins, which would result in overfitting of our model. Instead, we will come up with thresholds between values for our splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(dataset, labels, feature):\n",
    "    \n",
    "    #sort the indexes of the dataset for the given feature\n",
    "    sorted = np.argsort(dataset[:, feature])\n",
    "\n",
    "    #apply the sorting to the feature data and labels\n",
    "    sorted_data = dataset[sorted]\n",
    "    sorted_labels = labels[sorted]\n",
    "\n",
    "    #start at the top, calculating entropy level of the dataset\n",
    "    #to determine the level of purity\n",
    "    parent_entropy = calculate_entropy(labels)\n",
    "\n",
    "    #iterate over possible thresholds and get the indexes on each side\n",
    "    top_info_gain = 0\n",
    "    top_threshold = None\n",
    "    for i in range(1, len(sorted_data)):\n",
    "\n",
    "        threshold = (sorted_data[i-1, feature] + sorted_data[i, feature]) / 2\n",
    "        #left split is a boolean array containing True for the indexes with a value less than the threshold\n",
    "        #right split is a boolean array containing True for the indexes with a value greater than or equal to the threshold\n",
    "        left_split = sorted_data[:, feature] < threshold\n",
    "        right_split = sorted_data[:, feature] >= threshold\n",
    "\n",
    "        #next we calculate the entropies for the left and right sides of the split\n",
    "        #this will tell us how effective the split is, and we will maximize information gain\n",
    "        left_split_entropy = calculate_entropy(sorted_labels[left_split])\n",
    "        right_split_entropy = calculate_entropy(sorted_labels[right_split])\n",
    "\n",
    "        #calculate the weights of the left and right splits for use in the overall weight entropy calculation\n",
    "        weight_l = np.sum(left_split) / len(sorted_data)\n",
    "        weight_r = np.sum(right_split) / len(sorted_data)\n",
    "\n",
    "        #calculate the overall weighted entropy of the current split\n",
    "        weighted_split_entropy = weight_l * left_split_entropy + weight_r * right_split_entropy\n",
    "\n",
    "        #using the weighted split entropy, calculate the information gain of the threshold\n",
    "        #if the information gain is higher than any we've come across thus far, set as the top one\n",
    "        information_gain = parent_entropy - weighted_split_entropy\n",
    "        if information_gain > top_info_gain:\n",
    "            top_info_gain = information_gain\n",
    "            top_threshold = threshold\n",
    "\n",
    "    return top_info_gain, top_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our functions to calculate entropy and weighted information gain, we will create the Decision Tree Classifier itself. First, we need to create a Node class to store various attributes (feature index, threshold, left of threshold values, right of threshold values) of the node we are evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "\n",
    "        #feature: index of feature being used for split\n",
    "        self.feature = feature\n",
    "\n",
    "        #threshold: threshold for the split\n",
    "        self.threshold = threshold\n",
    "\n",
    "        #left: the left node, contains all values < threshold\n",
    "        self.left = left\n",
    "\n",
    "        #right: the right node, contains all values >= threshold\n",
    "        self.right = right\n",
    "\n",
    "        #value: value (0 or 1 for benign or malignant) assigned when a leaf node is reached\n",
    "        #remains None until leaf node is found, then returns the value\n",
    "        self.value = value\n",
    "\n",
    "    def leaf_node_reached(self):\n",
    "        if self.value is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our DecisionTreeClassifier class. This object contains all relevant methods for building our decision tree, recursively building tree nodes and calculating/maximizing information gain. I've annotated each method below with comments on its purpose: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, maximum_tree_depth):\n",
    "\n",
    "        #maximum_tree_depth (int): the maximum depth for our tree\n",
    "        self.maximum_tree_depth = maximum_tree_depth\n",
    "\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, dataset, labels):\n",
    "        #method containing all logic to train our classifier\n",
    "        self.tree = self.build_tree(dataset, labels)\n",
    "\n",
    "    def build_tree(self, dataset, labels, cur_tree_depth=0):\n",
    "        #this method is where the bulk of the work is handled for building our decision tree\n",
    "\n",
    "        #label count gives us the number of labels in the current sample set\n",
    "        #label count of 1 indicates perfect purity (one of our stopping criteria)\n",
    "        label_ct = len(np.unique(labels))\n",
    "\n",
    "        #we first check our stoppage criteria, to see if we've reached a leaf node\n",
    "        #if our current depth has hit the stoppage depth defined, OR\n",
    "        #if the label count has a value of 1\n",
    "        if cur_tree_depth >= self.maximum_tree_depth or label_ct == 1:\n",
    "            # Return a leaf node with the most common class label\n",
    "            leaf_value = self.get_top_label(labels)\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        #call get_top_split() to calculate the top feature and threshold\n",
    "        top_feature, top_threshold = self.get_top_split(dataset, labels)\n",
    "        if top_feature is None:\n",
    "            #no feature being returned indicates we've reached a leaf node\n",
    "            #return this TreeNode object with the leaf value found\n",
    "            leaf_value = self.get_top_label(labels)\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        #split data on top feature and threshold\n",
    "        #then build the sub trees off using this split\n",
    "        left_split = dataset[:, top_feature] < top_threshold\n",
    "        right_split = dataset[:, top_feature] >= top_threshold\n",
    "        left_sub_tree = self.build_tree(dataset[left_split], labels[left_split], cur_tree_depth + 1)\n",
    "        right_sub_tree = self.build_tree(dataset[right_split], labels[right_split], cur_tree_depth + 1)\n",
    "\n",
    "        return TreeNode(feature=top_feature, threshold=top_threshold, left=left_sub_tree, right=right_sub_tree)\n",
    "\n",
    "    def get_top_split(self, dataset, labels):\n",
    "\n",
    "        feature_ct = dataset.shape[1]\n",
    "        top_feature = None\n",
    "        top_threshold = None\n",
    "        top_info_gain = 0\n",
    "\n",
    "        #iterate over the features and determine which feature/threshold provide the highest information gain\n",
    "        for feature in range(feature_ct):\n",
    "            info_gain, threshold = calculate_information_gain(dataset, labels, feature)\n",
    "            if info_gain > top_info_gain:\n",
    "                top_info_gain = info_gain\n",
    "                top_feature = feature\n",
    "                top_threshold = threshold\n",
    "\n",
    "        return top_feature, top_threshold\n",
    "\n",
    "    def get_top_label(self, labels):\n",
    "        #returns the most common label in the array\n",
    "        #we call this function to determine whether benign or malignant samples are more prominent\n",
    "        return np.bincount(labels).argmax()\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        predictions = []\n",
    "        for point in dataset:\n",
    "            #append the predicted label value to our predictions list\n",
    "            predictions.append(self.predict_datapoint(point, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_datapoint(self, point, node):\n",
    "        #first check if we've reached a leaf node\n",
    "        #if so, return the determined label value\n",
    "        if node.leaf_node_reached():\n",
    "            return node.value\n",
    "        \n",
    "        #if not leaf value, continue down the left or right tree nodes\n",
    "        if point[node.feature] < node.threshold:\n",
    "            return self.predict_datapoint(point, node.left)\n",
    "        else:\n",
    "            return self.predict_datapoint(point, node.right)\n",
    "\n",
    "    #required getter and setter functions for our object \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"maximum_tree_depth\": self.maximum_tree_depth}\n",
    "\n",
    "    def set_params(self, maximum_tree_depth=None):\n",
    "        self.maximum_tree_depth = maximum_tree_depth\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the naive bayes classifer, data preprocessing first gets done to ensure only relevant data is utilized.  The ID feature will not be relevant in predicting the Diagnosis and gets dropped from the dataset.  Encoding our target variable turns the output to numeric values, making it easier to identify within this classifier.  The data set next has a 75/25 training and testing split to help the classifier learn before it gets tested.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Look into Target Column 0    M\n",
      "1    M\n",
      "2    M\n",
      "3    M\n",
      "4    M\n",
      "Name: Diagnosis, dtype: object\n",
      "Verification that encoding works: 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Diagnosis Encoding, dtype: int64\n",
      "Index(['radius1', 'texture1', 'perimeter1', 'area1', 'smoothness1',\n",
      "       'compactness1', 'concavity1', 'concave_points1', 'symmetry1',\n",
      "       'fractal_dimension1', 'radius2', 'texture2', 'perimeter2', 'area2',\n",
      "       'smoothness2', 'compactness2', 'concavity2', 'concave_points2',\n",
      "       'symmetry2', 'fractal_dimension2', 'radius3', 'texture3', 'perimeter3',\n",
      "       'area3', 'smoothness3', 'compactness3', 'concavity3', 'concave_points3',\n",
      "       'symmetry3', 'fractal_dimension3', 'Diagnosis Encoding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "print(f\"Initial Look into Target Column {data_df['Diagnosis'].head()}\")\n",
    "\n",
    "#Dropping Irrelevant ID column\n",
    "if 'ID' in data_df.columns:\n",
    "    data_df = data_df.drop(columns=['ID'])\n",
    "\n",
    "\n",
    " \n",
    "#Label Encoding for Diagnosis\n",
    "data_df['Diagnosis Encoding'] = pd.factorize(data_df['Diagnosis'])[0]\n",
    "data_df['Diagnosis Encoding']=data_df['Diagnosis Encoding'].add(1)\n",
    "\n",
    "#Malignant will show up as '1' AND Benign as '2'\n",
    "print(f\"Verification that encoding works: {data_df['Diagnosis Encoding'].head()}\")\n",
    "data_df = data_df.drop('Diagnosis', axis = 1)\n",
    "\n",
    "#Splitting Data\n",
    "train_data = data_df.sample(frac=0.75, random_state = 42)\n",
    "test_data = data_df.drop(train_data.index)\n",
    "x_train = train_data.drop(\"Diagnosis Encoding\", axis = 1)\n",
    "x_test = test_data.drop(\"Diagnosis Encoding\", axis = 1)\n",
    "y_train = train_data['Diagnosis Encoding']\n",
    "y_test = test_data['Diagnosis Encoding']\n",
    "\n",
    "\n",
    "print(data_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes for our given dataset is Benign and Malignant.  Naive Bayes makes the assumption that features are independent given a class label.  Because the intention is to predict likelihood on whether new data belongs to a specific class, the training dataset gets defined by whether our target variable (Diagnosis) gives a 1 or 2.  From the class dataset, the mean and variance methods are defined in preparation for the Gaussian probability function.  This method calculates the probability of each feature in a data sample/instance based on Gaussian distribution.  Traditional computation would combine these probabilites through finding their product.  Finding the logarithmic summation was done instead due to logarithmic function's ability to handle smaller values better.  Additionally, any values of zero that could be found in variance were redefined as a miniscule number 1E-12 to avoid possibility of diving by zero.  This same concept was applied to the probability of a feature value being found in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the data by class \n",
    "class Benign:\n",
    "    #Having benign data be instance method allows method to call on itself\n",
    "    def benign_data(self, x_train, y_train):\n",
    "        x_train_b = x_train[y_train == 2]\n",
    "        return x_train_b\n",
    "    def mean(self, x_train_b):\n",
    "        mean_value = {}\n",
    "        #Calculating mean of the numerical features in dataset\n",
    "        for column in x_train_b.select_dtypes(exclude ='object'):\n",
    "            mean_value[column] = x_train_b[column].mean()\n",
    "        return mean_value\n",
    "    def variance(self, x_train_b):\n",
    "        variance = {}\n",
    "        #Calculating variance of numerical features in databset\n",
    "        for column in x_train_b.select_dtypes(exclude ='object'):\n",
    "            #Recording the variance value for each individual feature\n",
    "            variance[column] = x_train_b[column].var()\n",
    "        return variance\n",
    "    #Probability calculation requires parameters of class data set, index being called plus pre-calculated mean & variance\n",
    "    def gaussianProbability(self, x_train_b, index, mean,variance):     \n",
    "        #Defining the probability sum before loop so function can add up all probabilities through each run in loop\n",
    "        sum_logP = 0\n",
    "        for column in x_train_b.select_dtypes(exclude ='object'):\n",
    "            #After parsing through a feature, x captures the data sample/instance within index that is called\n",
    "            x = x_train_b.loc[index,column]\n",
    "            μ = mean[column]\n",
    "            σ = variance[column]\n",
    "            #Handling zero values to avoid breaking Gaussian Probability calculation\n",
    "            if σ == 0:\n",
    "                σ = 1E-12\n",
    "            if μ ==0:\n",
    "                μ = 1E-12\n",
    "            #Setting up Naive Bayes proability calculation on continuous features by splitting numerator and denominator\n",
    "            #Assumes dataset has a Gaussian Distribution\n",
    "            num = math.exp(-(x-μ)**2/(2*σ))\n",
    "            denom = math.sqrt(2 * math.pi * σ)\n",
    "            P = num/denom\n",
    "            #Ensuring probability will not get too close to 0\n",
    "            if P <= 0:\n",
    "                P = 1E-18\n",
    "            #Using summation o\n",
    "            #Using summation of log function over product of posterior probability since log handles small numbers better\n",
    "            logP= (math.log(P))\n",
    "            #Variable increases the count of logP\n",
    "            sum_logP += logP\n",
    "        return sum_logP\n",
    "\n",
    "\n",
    "class Malignant:\n",
    "    def malignant_data(self, x_train, y_train):\n",
    "        x_train_m = x_train[y_train == 1]\n",
    "        return x_train_m\n",
    "    \n",
    "    def mean(self, x_train_m):\n",
    "        mean_value = {}\n",
    "        #Calculating mean of the numerical features in dataset\n",
    "        for column in x_train_m.select_dtypes(exclude ='object'):\n",
    "            mean_value[column] = x_train_m[column].mean()\n",
    "        return mean_value\n",
    "    def variance(self, x_train_m):\n",
    "        variance = {}\n",
    "        #Calculating variance of numerical features in databset\n",
    "        for column in x_train_m.select_dtypes(exclude ='object'):\n",
    "            #Recording the variance value for each individual feature\n",
    "            variance[column] = x_train_m[column].var()\n",
    "        return variance\n",
    "    #Probability calculation requires parameters of class data set, index being called plus pre-calculated mean & variance\n",
    "    def gaussianProbability(self, x_train_m, index, mean,variance):     \n",
    "        #Defining the probability sum before loop so function can add up all probabilities through each run in loop\n",
    "        sum_logP = 0\n",
    "        for column in x_train_m.select_dtypes(exclude ='object'):\n",
    "            #After parsing through a feature, x captures the data sample/instance within index that is called\n",
    "            x = x_train_m.loc[index,column]\n",
    "            μ = mean[column]\n",
    "            σ = variance[column]\n",
    "            #Handling zero values to avoid breaking Gaussian Probability calculation\n",
    "            if σ == 0:\n",
    "                σ = 1E-12\n",
    "            if μ ==0:\n",
    "                μ = 1E-12\n",
    "            #Setting up Naive Bayes proability calculation on continuous features by splitting numerator and denominator\n",
    "            #Assumes dataset has a Gaussian Distribution\n",
    "            num = math.exp(-(x-μ)**2/(2*σ))\n",
    "            denom = math.sqrt(2 * math.pi * σ)\n",
    "            P = num/denom \n",
    "            #Ensuring probability will not get too close to 0\n",
    "            if P <= 0:\n",
    "                P = 1E-18\n",
    "            #Using summation of log function over product of conditional probabilities since log handles small numbers better\n",
    "            logP= (math.log(P))\n",
    "            #Variable increases the count of logP\n",
    "            sum_logP += logP\n",
    "        return sum_logP\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a NaiveBayesClassifier object to help us classify our data samples as benign or malignant. Below is a breakdown of the class:\n",
    "\n",
    "- init(): initialize the object\n",
    "- fit(): the method for training our NB classifier. In this method, we calculate the prior probabilities for both malignant and benign label values. We then train the classifier based off of these prior beliefs, and calculate the mean and variance values for both the malignant and benign labels.\n",
    "- predict(): in this method, we compute the actual prediction value of the label for a given row of data in our test split dataset. We calculate the Gaussian probability for each class label (B vs. M). Whichever one is higher is the predictive value we assign to the data.\n",
    "- get_params() / set_params(): allows for getting and setting of the object's values dynamically. These two methods are required in order to work with SKLearn's cross_val_score function that we utilize in order to calculate the performance of our model (without these two methods, errors are thrown when trying to call the function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, priors=None):\n",
    "        self.benign = Benign()\n",
    "        self.malignant = Malignant()\n",
    "        self.priors = priors if priors is not None else {}\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        n_total = len(y_train)\n",
    "        n_benign = np.sum(y_train == 2)\n",
    "        n_malignant = np.sum(y_train == 1)\n",
    "\n",
    "        self.priors['benign'] = n_benign / n_total\n",
    "        self.priors['malignant'] = n_malignant / n_total\n",
    "\n",
    "        # Fit the Benign and Malignant models using the training data\n",
    "        self.x_train_b = self.benign.benign_data(x_train, y_train)\n",
    "        self.mean_b = self.benign.mean(self.x_train_b)\n",
    "        self.variance_b = self.benign.variance(self.x_train_b)\n",
    "\n",
    "        self.x_train_m = self.malignant.malignant_data(x_train, y_train)\n",
    "        self.mean_m = self.malignant.mean(self.x_train_m)\n",
    "        self.variance_m = self.malignant.variance(self.x_train_m)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        predictions = []\n",
    "\n",
    "        for index in x_test.index:\n",
    "            #Compute probabilities, and assign the predicted class of 2 (benign) or 1 (malignant)\n",
    "            #depending on which probability is higher\n",
    "            gauss_test_B = self.benign.gaussianProbability(x_test, index, self.mean_b, self.variance_b)\n",
    "            gauss_test_M = self.malignant.gaussianProbability(x_test, index, self.mean_m, self.variance_m)\n",
    "            pred_class = 2 if gauss_test_B > gauss_test_M else 1\n",
    "            predictions.append(pred_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'priors': self.priors}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our decision tree and naive bayes classifiers, we will begin the process of testing the performance and analyzing our results. Below, we've created two functions:\n",
    "\n",
    "1. test_performance() - this function has two arguments, X (the data matrix) and y (the label values). \n",
    "2. get_f1() - a helper function called by our test_performance function. This function takes four arguments - the data matrix and label values passed in from test_performance, along with the stratified k fold and classifier objects we've created. We return sklearn's cross_val_score call on our parameters, which returns an array containing the f1 scores at each fold in the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(classifier, X, y):\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    f1_scores = get_f1(X, y, kfold, classifier)\n",
    "\n",
    "    mean_f1_scores = np.mean(f1_scores)\n",
    "    stdev_f1_scores = np.std(f1_scores)\n",
    "\n",
    "    return mean_f1_scores, stdev_f1_scores\n",
    "\n",
    "def get_f1(X, y, skf, classifier):\n",
    "    return cross_val_score(classifier, X, y, cv=skf, scoring=make_scorer(f1_score, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've built our performance calculating functions, we want to plot our results. We call test_performance, and get the mean and standard deviation values of our f1 scores at each fold, for both of our classifiers. With these two values, we build up a plot to show each classifier's performance, along with the levels of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Mean: 0.9244216091945578\n",
      "Decision Tree F1 Std. Dev.: 0.03282990480395301\n",
      "Naive Bayes F1 Mean: 0.8985170935016136\n",
      "Naive Bayes F1 Std. Dev.: 0.06205971863970108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_performance(classifier_names, mean_f1, std_f1):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(classifier_names, mean_f1, yerr=std_f1, color=['blue', 'red'], capsize=10)\n",
    "    ax.set_ylabel('F1 Score (mean)')\n",
    "    ax.set_title('Classifier Performance')\n",
    "    plt.show()\n",
    "\n",
    "dec_tree_mean_f1, dec_tree_std_f1 = test_performance(\n",
    "    DecisionTreeClassifier(maximum_tree_depth=5), \n",
    "    features, \n",
    "    target)\n",
    "\n",
    "nb_mean_f1, nb_std_f1 = test_performance(\n",
    "    NaiveBayesClassifier(), \n",
    "    x_test, y_test)\n",
    "\n",
    "print(f\"Decision Tree F1 Mean: {dec_tree_mean_f1}\")\n",
    "print(f\"Decision Tree F1 Std. Dev.: {dec_tree_std_f1}\")\n",
    "print(f\"Naive Bayes F1 Mean: {nb_mean_f1}\")\n",
    "print(f\"Naive Bayes F1 Std. Dev.: {nb_std_f1}\")\n",
    "\n",
    "classifier_names = [\"Decision Tree\", \"Naive Bayes\"]\n",
    "f1_means = [dec_tree_mean_f1, nb_mean_f1]\n",
    "f1_stds = [dec_tree_std_f1, nb_std_f1]\n",
    "\n",
    "plot_performance(classifier_names, f1_means, f1_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we can see that the decision tree classifier has roughly a 93% mean F1 score, which indicates our classifier is effectively handling our imbalanced dataset and considering false positives and false negatives. The low standard deviation (roughly 0.03) tells us that the classifier is avoiding overfitting the data and performing well across the ten folds.  \n",
    "\n",
    "Naive Bayes is able to achieve a mean F1 score of 0.9061, which reveals that this classifier effectively handles the dataset.  Since the F1 tests false positive and negatives, the higher score suggests that this classifier has a minimal chance of error.  The small standard deviation of 0.0603744 shows that the classifier's performance is consistent with minimal variance.  This avoids overfitting and boosts confidence in ability to handle foreign datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection with Randomization\n",
    "\n",
    "20% of dataset is used to randomly shuffle all features\n",
    "80% dataset is untouched and used for testing at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of original dataset for random permutation\n",
    "\n",
    "#Method that loops through all columns and randomly permutates each feature\n",
    "def permutatingColumn(data):\n",
    "    if column in data.columns:\n",
    "        if column !='Diagnosis':\n",
    "            random_perm_data = data.copy()\n",
    "            random_perm_data[column] = np.random.permutation(random_perm_data[column])\n",
    "    return random_perm_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the datset to set up for model training.  As detailed in the instructions, one of the model trainings will follow our standard dataset while the second one incorporates 20% stratified random sampling and the remaining 80% for testing will consist purely of the original dataset.  In order to keep data consistent with the classifiers written from scratch, encoding of the target feature Diagnosis was applied on the randomly permutated dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Look into Target Column: \n",
      " 0    M\n",
      "1    M\n",
      "2    M\n",
      "3    M\n",
      "4    M\n",
      "Name: Diagnosis, dtype: object\n",
      "Verification that encoding works: \n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Diagnosis Encoding, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Feature selection on the 20% stratified random sample of entire dataset\n",
    "\n",
    "\n",
    "#Turning the randomly permutated datset into a dataframe for easy data splitting\n",
    "rp_dataDF = pd.DataFrame(rp_data)\n",
    "\n",
    "#Adding encoding feature to keep randomly stratified data consistent with original dataset\n",
    "print(f\"Initial Look into Target Column: \\n {rp_dataDF['Diagnosis'].head()}\")\n",
    "rp_dataDF['Diagnosis Encoding'] = pd.factorize(rp_dataDF['Diagnosis'])[0]\n",
    "rp_dataDF['Diagnosis Encoding']=rp_dataDF['Diagnosis Encoding'].add(1)\n",
    "rp_dataDF = rp_dataDF.drop('Diagnosis', axis=1)\n",
    "print(f\"Verification that encoding works: \\n{rp_dataDF['Diagnosis Encoding'].head()}\")\n",
    "#Malignant will show up as 1 AND Benign as 2\n",
    "\n",
    "#Training and Testing Data Split\n",
    "train_data_R = rp_dataDF.sample(frac=0.2, random_state = 41)\n",
    "test_data_R = data_df.sample(frac = 0.8, random_state = 41)\n",
    "x_train_R = train_data_R.drop(columns = ['Diagnosis Encoding'])\n",
    "x_test_R = test_data_R.drop(columns = ['Diagnosis Encoding'])\n",
    "y_train_R = train_data_R['Diagnosis Encoding']\n",
    "y_test_R = test_data_R['Diagnosis Encoding']\n",
    "\n",
    "\n",
    "x_train_R_dt = np.array(x_train_R)\n",
    "x_test_R_dt = np.array(x_test_R)\n",
    "y_train_R_dt = np.array(y_train_R)\n",
    "y_test_R_dt = np.array(y_test_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where we start training the model with data containing randomized features.  We start off by using Decision Tree classifier and then employ the Naive Bayes classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Mean: 0.9146649609314516\n",
      "Decision Tree F1 Std. Dev.: 0.04181485560919544\n",
      "Naive Bayes F1 Mean: 0.9287030835699415\n",
      "Naive Bayes F1 Std. Dev.: 0.05311624336411716\n"
     ]
    }
   ],
   "source": [
    "#Redefining test performance method with 5-fold cross validation\n",
    "def test_performance2(classifier, X, y):\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    f1_scores = get_f1(X, y, kfold, classifier)\n",
    "\n",
    "    mean_f1_scores = np.mean(f1_scores)\n",
    "    stdev_f1_scores = np.std(f1_scores)\n",
    "\n",
    "    return mean_f1_scores, stdev_f1_scores\n",
    "\n",
    "#Creating method that evaluates and compares both models against OG & randomized datasets\n",
    "def dual_model_performance(model1, model2, x_train, y_train, x_test, y_test, x_train_R, y_train_R, x_test_R, y_test_R):\n",
    "    #Evaluating Decision Tree model on original dataset\n",
    "    model1 = DecisionTreeClassifier(maximum_tree_depth=5)\n",
    "    model1.fit(x_train, y_train)\n",
    "    model1.build_tree(x_train, y_train, cur_tree_depth=0)\n",
    "    dec_tree_mean_f1, dec_tree_std_f1 = test_performance(\n",
    "        DecisionTreeClassifier(maximum_tree_depth=5), x_test, y_test)\n",
    "    \n",
    "    #Evaluating Decision Tree model on randomly permutated dataset\n",
    "    model_dt= DecisionTreeClassifier(maximum_tree_depth=5)\n",
    "    model_dt.fit(x_train_R_dt, y_train_R_dt)\n",
    "    model_dt.build_tree(x_train_R_dt, y_train_R_dt, cur_tree_depth=0)\n",
    "    dt_mean_f1, dt_std_f1 = test_performance(\n",
    "        DecisionTreeClassifier(maximum_tree_depth=5), x_test_R_dt, y_test_R_dt)\n",
    "\n",
    "    #Evaluating Naive Bayes on original dataset\n",
    "    model2 = NaiveBayesClassifier()\n",
    "    model2.fit(x_train, y_train)\n",
    "    nb_mean_f1, nb_std_f1 = test_performance(\n",
    "        NaiveBayesClassifier(), x_test, y_test)\n",
    "    \n",
    "    #Evaluating Naive Bayes model on randomly permutated dataset\n",
    "    model_nb = NaiveBayesClassifier()\n",
    "    model_nb.fit(x_train_R, y_train_R)\n",
    "    nB_mean_f1, nB_std_f1 = test_performance(\n",
    "        NaiveBayesClassifier(), x_test_R, y_test_R)\n",
    "\n",
    "#Generating a comparision dictionary to store results\n",
    "    comparisonModels = { \n",
    "        'decisionTree_original': dec_tree_mean_f1, \n",
    "        'decisionTree_original_std': dec_tree_std_f1, \n",
    "        'decisionTree_random': dt_mean_f1,\n",
    "        'decisionTree_random_std': dt_std_f1,\n",
    "        'naiveBayes_original': nb_mean_f1,\n",
    "        'naiveBayes_original_std': nb_std_f1,\n",
    "        'naiveBayes_random': nB_mean_f1,\n",
    "        'naiveBayes_random_std': nB_std_f1,\n",
    "        'decisionTree_difference': abs(dec_tree_mean_f1 - dt_mean_f1),\n",
    "        'naiveBayes_difference': abs(nb_mean_f1 - nB_mean_f1)\n",
    "    }\n",
    "\n",
    "    return comparisonModels\n",
    "\n",
    "print(f\"Decision Tree F1 Mean: {dt_mean_f1}\")\n",
    "print(f\"Decision Tree F1 Std. Dev.: {dt_std_f1}\")\n",
    "print(f\"Naive Bayes F1 Mean: {nB_mean_f1}\")\n",
    "print(f\"Naive Bayes F1 Std. Dev.: {nB_std_f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
